---
- name: 'Checks'
  import_playbook: 'checks.yml'

# Note: for HA embedded etcd, we MUST use an odd number of master nodes.
# https://docs.k3s.io/datastore/ha-embedded

# Work: while on libvirt8, run with "--skip-tags=broken_on_libvirt_8"

- name: 'Spread versions'
  hosts: 'all'
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  roles:
    # to circumvent know bugs in libvirtd :(
    - 'libvirtd_version_get'

# useful to find "empty" files: $ find roles/ -size 35c -type f
# useful to find actually empty files: $ find roles/ -size 0 -type f
# useful to find empty directories: $ find roles/ -empty -type d

# TODO #12 make vm - disk_setup, virsh_setup ?
# see roles/foreman_setup/templates/vmspec.xml.j2 (might be symlinks
# from other roles?)

- name: 'Basic setup of nodes'
  hosts: 'nodes'
  become: yes
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'nodes'
  roles:
    - role: 'xbps_setup'
      when:
        - 'has_xbps'
    - role: 'yum_setup'
      when:
        - 'has_yum'
    - role: 'apt_setup'
      when:
        - 'has_apt'
    - 'rsyslog_setup'
    - role: 'service_start'
      vars:
        services:
          - 'rsyslog'
    - 'logrotate_setup'
    - 'openssh_server_setup'
    - role: 'selinux_setup'
      when:
        - 'mandatory_access_control == "selinux"'
    - role: 'apparmor_setup'
      when:
        - 'mandatory_access_control == "apparmor"'
    - 'grub_setup'
    - 'services_tidy'
    - 'files_tidy'
    - role: 'iptables_setup'
      when:
        - 'has_iptables'  # is this right? maybe redesign this role
    - role: 'service_start'
      when:
        - 'has_iptables'
      vars:
        services:
          - 'iptables'
    - role: 'dhcpcd_setup'
      when:
        - 'has_dhcpcd'
    - role: 'service_start'
      when:
        - 'has_dhcpcd'
      vars:
        services:
          - 'dhcpcd'
    - role: 'ifcfg_setup'
      when:
        - 'not has_dhcpcd'
        - 'has_sysconfig'
    - role: 'networking_setup'
      when:
        - 'not has_dhcpcd'
        - 'not has_sysconfig'
    - 'chrony_setup'
# Ubuntu hates chrony apparently?
# TODO #13: fix
#    - role: 'service_start'
#      vars:
#        services:
#          - 'chronyd'
    #- 'do_nothing'
    #- 'fail_egregiously'

#- name: 'Prepare Foreman'
#  hosts: 'foreman'
#  become: yes
#  gather_facts: no
#  handlers:
#    - import_tasks: 'handlers/main.yml'
#  tags:
#    - 'foreman'
#    - 'never'  # remove 'never' when foreman_setup role is bulletproof
#  roles:
#    - 'foreman_setup'

- name: 'Prepare builder'
  hosts: 'builder'
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'builder'
    - 'never'  # only run when setting up the build host initially
  roles:
    - 'k3s_tarball_prep'
    - 'images_pull'

- name: 'Continue node preparation'
  hosts: 'nodes'
  become: yes
  gather_facts: yes
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'nodes'
    - 'prepare_nodes'
  roles:
    - 'limits_setup'
    - 'sysctl_setup'
    - 'swap_disable'
    - 'iscsi_setup'
    - role: 'service_start'
      vars:
        services:
          - 'iscsi'
          - 'iscsid'
    - 'hosts_file_setup'

- name: 'Install k3s'
  hosts: 'nodes'
  become: yes
  gather_facts: yes
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'nodes'
    - 'install_nodes'
  roles:
    - 'k3s_service_setup'
    - 'k3s_service_configure'

- name: 'Generate unique cluster token'
  hosts: 'primaries'
  become: yes
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  vars:
    services:
      - 'k3s'
  roles:
    - 'k3s_generate_token'
    - 'service_enable_and_reboot'

- name: 'Slurp cluster token'
  hosts: 'secondaries:agents'
  become: yes
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  vars:
    services:
      - 'k3s'
  roles:
    - 'k3s_service_add_token'
    - 'service_enable_and_reboot'

- name: 'Copy kube config to builder'
  hosts: 'primaries'
  connection: 'local'
  run_once: yes
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  roles:
    - 'k3s_copy_config'

- name: 'Generate API tokens'
  hosts: 'primaries'
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  roles:
    - 'nodes_check'
    - 'k3s_apitokens_generate'

- name: 'Install Helm'
  hosts: 'primaries'
  become: yes
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  roles:
    - 'helm_install'
    - 'coredns_patch'

- name: 'Install cluster infrastructure'
  hosts: 'primaries'
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  roles:
    - 'calico_install'
    - 'metallb_install'
    - 'ingress-nginx_install'
    - 'longhorn_install'

- name: 'Post-install auto-snapshot'
  hosts: 'foreman:nodes'
  connection: 'local'
  become: no
  gather_facts: no
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'foreman'
    - 'nodes'
    - 'snapshot'
  vars:
    snapshot_name: 'post-install-autosnap'
  roles:
    - 'snapshot_delta_commit'
    - 'snapshot_delta_create'

- name: 'Install apps'
  hosts: 'primaries'
  become: yes
  gather_facts: yes
  handlers:
    - import_tasks: 'handlers/main.yml'
  tags:
    - 'nodes'
    - 'install_apps'
  roles:
    - 'cert-manager_install'
    - 'benthos_install'
    - 'vault_install'
    - 'kerberos_install'
    - 'openldap_install'
    - 'postgresql_install'
    - 'prometheus_install'
    - 'grafana_install'
...
# vim: set filetype=yaml:
